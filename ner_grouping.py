# -*- coding: utf-8 -*-
"""ner_grouping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NzQYHXfNSQqBD8diV-rtCIe4wMfcL23F
"""
import os
from typing import Dict, List
import numpy as np
from functools import reduce
from util import save_pickle
from collections import defaultdict
from os.path import join as pjoin
import pickle
from tqdm import tqdm

SCORE_THRESH = 0.7

def verify_sub_words_are_consecutive(stack):
    indices = [el['index'] for el in stack]
    try:
        assert reduce(lambda holds_true_prev, curr_pair: holds_true_prev and (
            curr_pair[0] + 1 == curr_pair[1]), zip(indices, indices[1:]), True), "Not consecutive!"
    except AssertionError:
        import ipdb
        ipdb.set_trace()


def word_accumulator_fn(info_dict: Dict[str, List], curr_tok):
    if curr_tok['entity'].startswith('I'):
        if len(info_dict['curr_stack']) == 0:
            return info_dict  # just don't modify it
        info_dict['curr_stack'].append(curr_tok)
    else:  # starts with B
        assert curr_tok['entity'].startswith(
            'B'), "THERE IS SOME SUSSY STUFF GOING ON WITH THE TAGGER. OUTPUT OTHER THAN I- OR B-???"
        info_dict['word_list'].append(info_dict['curr_stack'])
        info_dict['curr_stack'] = [curr_tok]
    return info_dict


def condense_stack(stack):
    condensed_word = ''.join([part['word'][2:] if part['word'].startswith(
        "##") else ' ' + part['word'] for part in stack]).lstrip()
    scores = np.array([part['score'] for part in stack])
    return condensed_word, scores


def split_one_stack(stack):
    '''Processes only 1 stack and returns a list of untangled stacks (1 stack = 1 entity)'''
    # split section of indices into partitions
    current_ind = -1000
    partitions = []
    current_partition = []
    for el in stack:
        if el['index'] != current_ind + 1:
            # time for new partition
            if len(current_partition) != 0:
                partitions.append(current_partition)
            current_partition = [el]
        else:
            current_partition.append(el)
        current_ind = el['index']

    if len(current_partition) != 0:
        partitions.append(current_partition)

    return partitions


def split_stacks(stacks):
    '''Processes the stacks to return the correct entity stacks. (Untangles multiple entities in 1 stack to ensure each stack = 1 entity)'''
    # sometimes there will be multiple "entities" grouped in 1 stack but with indices that are not contiguous
    return [stack_splitted for stack in stacks for stack_splitted in split_one_stack(stack)]

def valid_fn(element,score_thresh=SCORE_THRESH):
  text, scores = element
  return text != '' and '[PAD]' not in text and scores.mean() > 0.9
def get_tokens_from_ner_specific(raw_outputs, entity_type):
    '''gets proper tokens from nlp pipeline (merges subwords as well)'''
    # output is a list of {entity : str, score : float, index : int, word : str, start : int, end : int}
    # goal: merge consecutive indices into one word
    outputs = filter(lambda output: output['entity'].endswith(
        entity_type), raw_outputs)
    try:
        word_stack = reduce(word_accumulator_fn, outputs,
                            dict(word_list=[], curr_stack=[]))
        all_stacks = word_stack['word_list']
    except Exception as e:
        print("exception:", str(e))
        import ipdb
        ipdb.set_trace()

    if len(word_stack['curr_stack']) > 0:
        all_stacks.append(word_stack['curr_stack'])
    # little assert statement for sanity check
    clean_stacks = split_stacks(all_stacks)
    [verify_sub_words_are_consecutive(stack) for stack in clean_stacks]
    return list(map(lambda el: el[0],filter(valid_fn, map(condense_stack, clean_stacks))))


def get_tokens_from_ner(raw_outputs, entity_list=['ADR', 'DRUG']):
    return {entity_type: get_tokens_from_ner_specific(raw_outputs, entity_type) for entity_type in entity_list}

files_to_load = ['preds.pickle','id_mappings.pickle']
def load_pickle(filename, dir):
    with open(pjoin(dir, filename), "rb") as f:
        return pickle.load(f)
def load_pickles(filenames,dir):
  return [load_pickle(filename,dir) for filename in filenames]

def read_data(data_dir):
  preds, id_mappings = load_pickles(['preds.pickle','id_mappings.pickle'],dir=data_dir)
  processed_preds = [(get_tokens_from_ner(pred), id_map) for pred, id_map in tqdm(zip(preds,id_mappings),total=len(preds)) if len(pred) != 0]
  return [(pred,id_map) for pred, id_map in processed_preds if len(pred['ADR']) != 0 or len(pred['DRUG']) != 0]



def groupby_rowid(data_all):
  row_id_dict = defaultdict(lambda : dict())
  for pred, indices in data_all:
    row_id, sentence_id = indices
    row_id_dict[row_id][sentence_id] = pred
  return row_id_dict

data_dirs = ['data/output-n2c2']
data_all = [entity_list for data_dir in data_dirs for entity_list in read_data(data_dir)]
groupedby = groupby_rowid(data_all)

save_pickle(dict(groupedby),"groupedby_n2c2.pickle")


